Krystal Wu
Pascal Wallisch
Principles of Data Science
27 April 2024
Capstone Project - Spotify Song Analysis
Introduction
For this project, I seeded my RNG with my N number using numpy.random.seed. The steps I took for data preprocessing are as follows:
1. Reading Data: The dataset of 52,000 songs is stored in a Pandas DataFrame using the 'read_csv' method. This DataFrame is assigned to a variable named 'data'.
2. Data Cleaning: I checked the dataset for null, missing, and duplicate values. The dataset contains none, eliminating the need to clean them with either imputation or element-wise/row-wise removal. I also considered the units of measurement for musical features to determine whether normalization/standardization would be necessary beforehand. As all 8 descriptive features are measured on a common scale of 0 to 1, and all others (e.g. tempo, duration) use units of measurement adhering to the real world (e.g. BPM, milliseconds) I found this unnecessary.
Some songs had a popularity score of 0. Upon reviewing Spotify's API, 'popularity' corresponds to the recent number of streams garnered relative to other artists. Even if a song receives such a low value, I considered them important to include for robustness of analysis and understanding of underrepresented songs on the Spotify platform.
3. Dimension Reduction: PCA was utilized in question (8) to extract the most meaningful principal component(s) from the data, then in question (10) to perform logistic regression upon those components. Again, I made sure to z-score my data before applying PCA so that the first principal component would not point to the mean, and to accurately use the Kaiser criterion for question (8). 


Project Questions
1. To determine if any of the 10 song features are reasonably normally distributed, I firstly created histograms for each feature. Then, I created Q-Q plots for each feature, which compare the quantiles of each feature's data against the quantiles of a theoretical normal distribution. If the points on the Q-Q plot fall approximately along the diagonal line, it suggests that the data is normally distributed. 
  

Figure 1: Histograms of Song Features
  

Figure 2: Q-Q Plots of Song Features


From the histograms and Q-Q plots, it appears that none of the features exhibit a perfectly normal distribution. However, exactly 2 features (danceability and tempo) are approximately normally distributed. 
2. To explore the relationship between song length and popularity, I measured the association between "duration" and "popularity" using both Pearson and Spearman's correlation coefficients. I also included a scatterplot for a visual representation of their relationship, in case either correlation coefficient could not accurately capture it. 
  

Figure 3: Scatterplot of Song Length vs. Popularity
  

Figure 4: Correlation Coefficients for Song Length vs. Popularity


I obtained a Pearson coefficient of ~ -0.055 and a Spearman coefficient of ~ -0.037, indicating a very weak negative correlation between song length and popularity. Additionally, the scatterplot indicated no visual pattern for the relationship between song length and popularity. As such, I concluded that there is no discernable relationship between song length and popularity. 
3. For this question, I explore the relationship between 'popularity' (numerical) and 'explicit' (categorical). Since 'explicit' is a Boolean/categorical value, it would not be meaningful for us to reduce it to sample means. Moreover, we are primarily interested in comparing the distributions of a numerical variable (popularity) between two independent groups (explicit VS non-explicit). As such, I determined a non-parametric significance test would be best, specifically the Mann-Whitney U Test. The null hypothesis is that there is no significant difference in popularity between explicit and non-explicit songs. 
  

Figure 5: Mann-Whitney U Test for Popularity VS Explicit


The test yielded a test statistic of 139361273.5 and a p-value of 3.07*10^-19. The p-value around 3.07 x 10^-19 is significantly lower than the chosen significance level of 0.05, indicating strong evidence against the null hypothesis. Therefore, we reject the null hypothesis and conclude that there is a significant difference in popularity between explicit and non-explicit songs. 
4. For this question, I explore the relationship between 'popularity' (numerical) and 'mode' (categorical). I did a Mann-Whitney U Test for the same reasons as Question (4). 
  

Figure 6: Mann-Whitney U Test for Popularity VS Mode


The test yielded a test statistic of 325452746 and a p-value around 2.02*10^-6. This is significantly lower than the chosen significance level of 0.05, indicating strong evidence against the null hypothesis. Therefore, we reject the null hypothesis and conclude that there is a significant difference in popularity between songs in major key and songs in minor key. 
5. This question suggests that a correlation between two quantitative variables (energy and loudness) is plausible. To verify this, I created a scatterplot of the two features, then measured the Pearson and Spearman coefficients. The scatterplot exhibited a clear trend between the two features. I obtained a Pearson coefficient of ~0.77 and a Spearman coefficient of ~0.73, indicating that there is indeed a strong and positive association between energy and loudness. Inspecting the correlation heatmap from EDA, there also doesn't appear to be any confounding features that would raise concern about the relationship.
  

Figure 7: Scatterplot of Energy VS Loudness
  

Figure 8: Correlation Coefficients of Energy VS Loudness


However, it would be misleading to interpret loudness as the causal effect of energy, or vice versa. A causal relationship between the two cannot be established without performing an experiment. 
6. To determine the song feature which best predicts popularity, I used a for-loop to create a simple linear regression model for each of the 10 features. I made sure to perform cross-validation before regression each time in order to prevent overfitting. I printed the R-squared, RMSE, intercept, and coefficient of each model. Of all features, I looked for the one with the highest R-squared value, as this indicates that a large portion of the variability in popularity can be explained by that feature. I also looked for the one with the lowest RMSE value, as this indicates that the model's predictions are close to the actual popularity values on average. 
  

Figure 9: Examples of Linear Regression Outputs
  

Figure 10: Best R-Squared and RMSE Values


Instrumentalness had both the highest R-squared value (~0.021, meaning that it accounts for ~2.1% of the variability in popularity) and lowest RMSE value (~21.457). Thus, I concluded that instrumentalness is the best feature. However, the low R-squared value suggests that it is not the best predictor.
7. For this question, I created a multiple regression model. All 10 features were assigned as predictors for the outcome of popularity. To address concerns of collinearity and overfitting, I again made sure to use cross-validation before. The results of multiple regression are below:
  

Figure 11: Multiple Regression Results for 10 Features VS Popularity


This model accounts for a larger proportion of variance than the one in (6)–albeit still not very good–with an R-squared value of ~0.05 (meaning the 10 features altogether accounts for ~5% of the variability in popularity) and a slightly lower RMSE value of 21.11. With all 10 features included, however, collinearity and/or overfitting is still likely a concern in this model. The higher R-squared may suggest this to be a better model of popularity, but it is not guaranteed.
8. I performed PCA to extract principal components from the data. As explained previously, I made sure to z-score the data before. I then calculated the Eigenvalues of the components and created a scree plot, with a red dotted line marking the threshold of 1.0 that is set by the Kaiser criterion. Using this criterion, I filtered the components and found that there are exactly 3 meaningful components (Eigenvalues>=1.0). 
  

Figure 12: Scree Plot from PCA
Then, I performed multiple regression using these 3 components as predictors, and popularity as the outcome. I then calculated the beta coefficients and R-squared values of all 3 components. 
  

Figure 13: Betas and Coefficients from Meaningful Components


Altogether, the r-squared value of ~0.0047 signifies that the components account for around 0.47% of the variance in popularity.
9. I set 'valence' as the predictor X, and 'mode' as the predictor Y. Because I am predicting a binary outcome, I chose to use a logistic regression model for classification. Then, I created an ROC curve to visualize the accuracy and AUC of the model. 
  

Figure 14: ROC Curve for Valence to Predict Mode


The AUC is a value of 0.50, and the ROC curve lies almost exactly upon the red diagonal line. This strongly indicates that there is no ability to discern whether a song is major or minor from valence. To find a better predictor, I used a for-loop to create logistic regression models for all 10 song features and measure their AUC values. The feature which achieved the maximum AUC value, 0.57, is attributed to speechiness. The ROC curve for this is below, showing a larger curve above the red diagonal. This indicates improved prediction capability compared to the one for valence:
  

Figure 15: ROC Curve for Speechiness to Predict Mode


10. I converted whether a song is classical or not as a binary outcome (1=classical, 0=not classical), then set this value to a new column in the DataFrame named 'is_classical'. I then set 'duration' as one predictor and the 3 principal components from question (8) as another, performed a PCA on both individually, and created separate logistic regression models for each predictor. 
  

Figure 16: AUC values for Duration VS Classical <> Principal Components VS Classical
  

  

Figure 17: ROC Curve for Duration <> ROC Curve for Principal Components


The principal components achieved a higher AUC value and a larger ROC curve, indicating that they are a much better predictor of whether a song is classical or not.


Extra Credit
For extra credit, I analyzed song titles in the data using scikit-learn and NLTK. Firstly, I used stopwords and PorterStemmer from NLTK in the preprocess_text function, where I remove punctuation from song titles, tokenize the text, remove stopwords, and stem each word. Then, I used sklearn's CountVectorizer to convert the preprocessed song titles into a matrix of token counts. This makes a bag-of-words representation of the data. I store the titles into a DataFrame where rows represent titles and columns represent terms after preprocessing. Finally, I sum along the rows to find the frequency of each term across all titles. 
  

Figure 18: Top 20 Most Frequent Terms in Song Titles


Assuming that 'remix' and 'feat' describe remixes of other songs and specific artist features, there seems to be quite a few songs about love. Many other insights, such as the prevalence of Christmas songs, can also be garnered from this analysis.